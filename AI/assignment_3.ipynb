{"cells":[{"cell_type":"markdown","metadata":{"id":"A9RfhUxKJCyy"},"source":["# Q1"]},{"cell_type":"markdown","metadata":{"id":"EgpbAVOuJCy0"},"source":["## Code for (1)"]},{"cell_type":"markdown","source":[],"metadata":{"id":"oFYFyZ19KurG"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"4F9M6pI9JCy0","executionInfo":{"status":"ok","timestamp":1709542424781,"user_tz":-480,"elapsed":6935,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}}},"outputs":[],"source":["# Setup environment\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","###from torch.nn.modules.activation import ReLU\n","\n","torch.manual_seed(1)\n","\n","# Build MLP\n","class MLP(nn.Module):\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","        #self.flatten=nn.Flatten()\n","\n","\n","        self.fc1=nn.Linear(28*28,350)\n","        self.relu1=nn.ReLU()\n","        self.fc2=nn.Linear(350,150)\n","        self.relu2=nn.ReLU()\n","        self.fc3=nn.Linear(150,10)\n","\n","\n","\n","\n","        #######################\n","        # TODO: initialize neural network components\n","\n","\n","\n","\n","        #######################\n","\n","    def forward(self, x):\n","\n","        #######################\n","        # TODO: define forwarding of MLP\n","        #x=self.flatten(x)\n","        #logits =self.linear_relu_stack(x)\n","        x=torch.flatten(x,1)\n","\n","        x=self.fc1(x)\n","        x=self.relu1(x)\n","        x=self.fc2(x)\n","        x=self.relu2(x)\n","        x=self.fc3(x)\n","        return x\n","\n","\n","\n","\n","        #######################\n","        ####return x I commentedd this"]},{"cell_type":"code","source":["if __name__=='__main__':\n","  mlp = MLP()\n","  print(f'The MLP structure you built is as follow: \\n{mlp}')"],"metadata":{"id":"CN66jq-zXrGc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709542424782,"user_tz":-480,"elapsed":4,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}},"outputId":"44d8a0bd-a26b-4c48-d59e-5e2c90874b83"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["The MLP structure you built is as follow: \n","MLP(\n","  (fc1): Linear(in_features=784, out_features=350, bias=True)\n","  (relu1): ReLU()\n","  (fc2): Linear(in_features=350, out_features=150, bias=True)\n","  (relu2): ReLU()\n","  (fc3): Linear(in_features=150, out_features=10, bias=True)\n",")\n"]}]},{"cell_type":"markdown","source":["## Code for (2)\n","Note: run this code after code for (1)"],"metadata":{"id":"Qfcew0bsJPeW"}},{"cell_type":"code","source":["# Calculate number of parameters\n","def count_parameters(model):\n","    param_dict = {}\n","    #######################\n","    # TODO: Iterate the model parameters and their names, and save them into dictionary param_dict with the following format -- name: number of param\n","    for name,parameter in model.named_parameters():\n","      if not parameter.requires_grad:\n","            continue\n","      param_dict[name]=parameter.numel()\n","\n","    #######################\n","    return param_dict"],"metadata":{"id":"TiDq03E1JOFD","executionInfo":{"status":"ok","timestamp":1709542424782,"user_tz":-480,"elapsed":3,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["if __name__=='__main__':\n","  mlp = MLP()\n","  print(count_parameters(mlp))"],"metadata":{"id":"vSWAIkwbX6o5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709542424782,"user_tz":-480,"elapsed":3,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}},"outputId":"3fcbf335-99fe-4aba-b22d-48406913bdb7"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["{'fc1.weight': 274400, 'fc1.bias': 350, 'fc2.weight': 52500, 'fc2.bias': 150, 'fc3.weight': 1500, 'fc3.bias': 10}\n"]}]},{"cell_type":"markdown","metadata":{"id":"eUklizTZJCy0"},"source":["## Code for (3)\n","Note: run this code after code for (1)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"c_gjkfFfJCy1","executionInfo":{"status":"ok","timestamp":1709542426746,"user_tz":-480,"elapsed":1966,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}}},"outputs":[],"source":["# Setup environment\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","def train(model):\n","  torch.manual_seed(1)\n","\n","  # Prepare dataset\n","  train_set = torchvision.datasets.MNIST(\"data/\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n","  test_set = torchvision.datasets.MNIST(\"data/\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n","\n","  #######################\n","  # TODO: define train_loader, test_loader using train_set, test_set, the utils.data.DataLoader function\n","  train_loader = DataLoader(train_set,batch_size=25,shuffle=True)\n","  test_loader = DataLoader(test_set,batch_size=1000,shuffle=True)\n","  #######################\n","\n","  # Build MLP\n","  mlp = model\n","  #print(f'The MLP structure you built is as follow: \\n{mlp}')\n","\n","  # Train MLP\n","  lossFunc = nn.CrossEntropyLoss()\n","  optimizer = optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9)\n","\n","  #######################\n","  # TODO: Train the MLP for 3 epochs\n","  epochs=3\n","  for i in range(epochs):\n","    for batch,(X,y) in enumerate(train_loader):\n","      prediction=model(X)\n","      loss=lossFunc(prediction,y)\n","      loss.backward()\n","      optimizer.step()\n","      optimizer.zero_grad()\n","\n","\n","  #######################"]},{"cell_type":"code","source":["def evaluate(model):\n","    # toggle evaluation mode\n","    model.eval()\n","    results = {}\n","\n","    # Prepare dataset\n","    train_set = torchvision.datasets.MNIST(\"data/\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n","    test_set = torchvision.datasets.MNIST(\"data/\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n","\n","    #######################\n","    # TODO: define train_loader, test_loader using train_set, test_set, the utils.data.DataLoader function\n","    train_loader = DataLoader(train_set,batch_size=25,shuffle=True)\n","    test_loader = DataLoader(test_set,batch_size=1000,shuffle=True)\n","    #######################\n","\n","    # define an evaluation function\n","    def evaluate_a_model(model, dataloader):\n","        correct = 0\n","        with torch.no_grad():\n","            #######################\n","            # TODO: Calculate the number of correctly predicted samples and store the number into variable \"correct\"\n","          for X,y in dataloader:\n","            prediction=model(X)\n","            correct+=sum((prediction.argmax(1)==y))#.type(torch.float).sum().item()\n","\n","\n","            #######################\n","        accuracy = 100.*correct / len(dataloader.dataset)\n","        return accuracy\n","\n","    # evaluate on training data\n","    train_acc = evaluate_a_model(model, train_loader)\n","    # evaluate on testing data\n","    test_acc = evaluate_a_model(model, test_loader)\n","    results['Final training accuracy'] = round(train_acc.item(), 2)\n","    results['Final testing accuracy'] = round(test_acc.item(), 2)\n","    return results"],"metadata":{"id":"YZ_E1umWX_b9","executionInfo":{"status":"ok","timestamp":1709542426746,"user_tz":-480,"elapsed":3,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["if __name__=='__main__':\n","    mlp = MLP()\n","    train(mlp)\n","    results = evaluate(mlp)\n","    print(results)"],"metadata":{"id":"ElGF0MBMYPYU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709542510680,"user_tz":-480,"elapsed":83936,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}},"outputId":"675b84bf-7d50-45f0-c7be-a66c2adec1d3"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 139309649.96it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 51241833.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 44175756.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 10769094.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n","{'Final training accuracy': 98.53, 'Final testing accuracy': 97.4}\n"]}]},{"cell_type":"markdown","metadata":{"id":"q-KVP9P-YzCP"},"source":["# Q2"]},{"cell_type":"markdown","source":["## Code for (1)"],"metadata":{"id":"Tn2Xx2ObtHDK"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"OHZnPL63YzCa","executionInfo":{"status":"ok","timestamp":1709542510680,"user_tz":-480,"elapsed":4,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}}},"outputs":[],"source":["# Setup environment\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","torch.manual_seed(1)\n","\n","# Build Convolutional Neural Network (CNN)\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","\n","        self.cov1=nn.Conv2d(1,32,kernel_size=(3,3),stride=(1,1),padding=1)\n","        self.relu1=nn.ReLU()\n","\n","        self.max_pool1=nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n","        self.cov2=nn.Conv2d(32,64,kernel_size=(3,3),stride=(1,1),padding=1)\n","        self.relu2=nn.ReLU()\n","        self.max_pool2=nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n","        self.fc1=nn.Linear(in_features=3136,out_features=128)\n","        self.relu3=nn.ReLU()\n","        self.fc2=nn.Linear(in_features=128,out_features=10)\n","\n","\n","\n","\n","        \"\"\"self.cov1=nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,padding_mode='zeros')\n","        self.cov2=nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding_mode='zeros')\n","\n","        self.max_pool=nn.MaxPool2d(kernel_size=2,stride=2)\n","        self.fc1=nn.Linear(in_features=64*5*5,out_features=128)\n","        self.fc2=nn.Linear(in_features=128,out_features=10)\"\"\"\n","\n","        #######################\n","        # TODO: initialize neural network components\n","\n","\n","\n","\n","        #######################\n","\n","\n","    def forward(self, x):\n","\n","        #######################\n","        # TODO: define forwarding for CNN\n","\n","        x=self.cov1(x)\n","        x=self.relu1(x)\n","        x=self.max_pool1(x)\n","\n","        x=self.cov2(x)\n","        x=self.relu2(x)\n","\n","        x=self.max_pool2(x)\n","\n","\n","        x=torch.flatten(x,start_dim=1)\n","\n","        x=self.fc1(x)\n","        x=self.relu3(x)\n","\n","        x=self.fc2(x)\n","\n","\n","\n","\n","        #######################\n","\n","        return x"]},{"cell_type":"code","source":["if __name__=='__main__':\n","  cnn = CNN()\n","  print(f'The CNN structure you built is as follow: \\n{cnn}')"],"metadata":{"id":"N5kRJ0LVtscd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709542510680,"user_tz":-480,"elapsed":3,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}},"outputId":"d4e747e7-9aae-47e8-dd3d-ed46daeeb1a9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["The CNN structure you built is as follow: \n","CNN(\n","  (cov1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (relu1): ReLU()\n","  (max_pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  (cov2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (relu2): ReLU()\n","  (max_pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n","  (relu3): ReLU()\n","  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",")\n"]}]},{"cell_type":"markdown","source":["## Code for (2)"],"metadata":{"id":"61z4Y9BwzInm"}},{"cell_type":"code","source":["def count_total_parameters(model):\n","    total_params = 0\n","    #######################\n","    # TODO: Iterate the parameters in CNN; Save the number of learnable param, and add it to total_params\n","    for name,parameter in model.named_parameters():\n","      if not parameter.requires_grad:\n","            continue\n","      total_params+=parameter.numel()\n","\n","    #######################\n","    return total_params"],"metadata":{"id":"UQwfXdv-zK6Z","executionInfo":{"status":"ok","timestamp":1709542510680,"user_tz":-480,"elapsed":2,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["if __name__=='__main__':\n","  cnn = CNN()\n","  total_params = count_total_parameters(cnn)\n","  print(f'Total number of learnable parameters: {total_params}')"],"metadata":{"id":"9ho6PqB8zLe8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709542510680,"user_tz":-480,"elapsed":2,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}},"outputId":"8ccff759-b108-46f6-9810-a3805b03c650"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of learnable parameters: 421642\n"]}]},{"cell_type":"markdown","metadata":{"id":"zLJphw5tJCy1"},"source":["## Code for (3)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"yVc1AaWkJCy1","executionInfo":{"status":"ok","timestamp":1709542511167,"user_tz":-480,"elapsed":489,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}}},"outputs":[],"source":["# Setup environment\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","def train_cnn(model):\n","  torch.manual_seed(1)\n","\n","  # Prepare dataset\n","  train_set = torchvision.datasets.MNIST(\"data/\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n","  test_set = torchvision.datasets.MNIST(\"data/\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n","\n","  #######################\n","  # TODO: define the train_loader, test_loader using train_set, test_set, the utils.data.DataLoader function\n","  train_loader = DataLoader(train_set,batch_size=200,shuffle=True)\n","  test_loader = DataLoader(test_set,batch_size=1000,shuffle=True)\n","  #######################\n","\n","\n","  # Build Convolutional Neural Network (CNN)\n","  cnn = model\n","  #print(f'The CNN structure you built is as follow: \\n{cnn}')\n","\n","  # Train CNN\n","  lossFunc = nn.CrossEntropyLoss()\n","  optimizer = optim.SGD(cnn.parameters(), lr=0.01, momentum=0.9)\n","\n","  #######################\n","  # TODO: Train the CNN for 5 epochs\n","  epochs=3\n","  for i in range(epochs):\n","    for batch,(X,y) in enumerate(train_loader):\n","      prediction=model(X)\n","      loss=lossFunc(prediction,y)\n","      loss.backward()\n","      optimizer.step()\n","      optimizer.zero_grad()\n","\n","\n","\n","  #######################"]},{"cell_type":"code","source":["def evaluate_cnn(model):\n","    # toggle evaluation mode\n","    model.eval()\n","    results = {}\n","\n","    # Prepare dataset\n","    train_set = torchvision.datasets.MNIST(\"data/\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n","    test_set = torchvision.datasets.MNIST(\"data/\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n","\n","    #######################\n","    # TODO: define train_loader, test_loader using train_set, test_set, the utils.data.DataLoader function\n","    train_loader = DataLoader(train_set,batch_size=200,shuffle=True)\n","    test_loader = DataLoader(test_set,batch_size=1000,shuffle=True)\n","    #######################\n","\n","    # define an evaluation function\n","    def evaluate_a_model(model, dataloader):\n","        correct = 0\n","        with torch.no_grad():\n","          for X,y in dataloader:\n","            prediction=model(X)\n","            correct+=sum((prediction.argmax(1)==y))\n","            #######################\n","            # TODO: Calculate the number of correctly predicted samples and store the number into variable \"correct\"\n","\n","\n","            #######################\n","        accuracy = 100.*correct / len(dataloader.dataset)\n","        return accuracy\n","\n","    # evaluate on training data\n","    train_acc = evaluate_a_model(model, train_loader)\n","    # evaluate on testing data\n","    test_acc = evaluate_a_model(model, test_loader)\n","    results['Final training accuracy'] = round(train_acc.item(), 2)\n","    results['Final testing accuracy'] = round(test_acc.item(), 2)\n","    return results"],"metadata":{"id":"Tc8dGuM-1HTB","executionInfo":{"status":"ok","timestamp":1709542511167,"user_tz":-480,"elapsed":2,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["if __name__=='__main__':\n","    cnn = CNN()\n","    train_cnn(cnn)\n","    results = evaluate_cnn(cnn)\n","    print(results)"],"metadata":{"id":"u5OCy0GzMrEh","colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"status":"error","timestamp":1709542532997,"user_tz":-480,"elapsed":21831,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}},"outputId":"0d42cc85-2894-4c6a-d913-4a45b01a878e"},"execution_count":14,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-4490fb1c4cb1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-6a49db93147f>\u001b[0m in \u001b[0;36mtrain_cnn\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlossFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["# Q3"],"metadata":{"id":"49_-9fKFPMZ_"}},{"cell_type":"markdown","source":["### Prepare the dataset"],"metadata":{"id":"JJcN9oO3Lhrd"}},{"cell_type":"code","source":["import seaborn as sns\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import scipy\n","import sklearn"],"metadata":{"id":"zx4XskusYzMu","executionInfo":{"status":"aborted","timestamp":1709542532998,"user_tz":-480,"elapsed":4,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1klBpJLHNfFF","executionInfo":{"status":"aborted","timestamp":1709542532998,"user_tz":-480,"elapsed":4,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Read Dataset\n","Now you have the needed libraries in hand. Next, let's read the dataset from the source file to the project.  \n","\n","We assume you are working in Google Colab. One way to read a dataset in Google Colab:\n","1. Download the source file and put it on your Google Drive\n","2. Import the `drive` module from `google.colab` package\n","3. Run `drive.mount` to mount your Google Drive to the Colab notebook\n","4. Use `pandas.read_csv` to read the data from Google Drive and store the data in pandas DataFrame\n","\n","Todo:\n","Modify `YourFilePath` depending on the actual directory to read the data to this notebook.\n","\n","Remarks:  \n","You can check whether your data reading is successful by running the next cell. The shape should be (4892, 12). You can also see the first 5 rows of the dataset."],"metadata":{"id":"bSnurZXzqbt0"}},{"cell_type":"code","source":["if __name__ == '__main__':\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","# todo start #\n","# please modify YourFilePath\n","    data_file = \"/content/drive/MyDrive/comp4331/heart_failure_clinical_records_dataset.csv\"\n","    # data_df = pd.read_csv(data_file)\n","    # data_df.insert(0, 'patient_id', range(1, 1 + len(data_df)))\n","    data_df = pd.read_csv(data_file)\n","    data_df.insert(0, 'patient_id', range(1, 1 + len(data_df)))\n","# todo end #\n","    print(data_df.head(10))"],"metadata":{"id":"1u0Nr4npbBbB","executionInfo":{"status":"aborted","timestamp":1709542532998,"user_tz":-480,"elapsed":3,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Code for (1)"],"metadata":{"id":"jmYQjiI-PPAm"}},{"cell_type":"code","source":["def kmeans(X, n_clusters):\n","\n","    results = {}\n","\n","    rng = np.random.RandomState(42)\n","    random_idx = rng.permutation(X.shape[0])[:n_clusters]\n","\n","    ###########################################\n","    # TODO: Step 1: Randomly initialize the centroids\n","    centroids = X[random_idx]\n","    #print(type(centroids))\n","    X = np.array(list(X))\n","\n","\n","\n","    ###########################################\n","\n","    while True:\n","        ###########################################\n","        # TODO: Step 2: Assign each data point to the closest centroid\n","        # Hint: labels is a numpy array of shape (n_samples,), where n_samples is the number of samples in the input data X.\n","        #       Each element in labels is an integer that represents the cluster index to which the corresponding sample in X is assigned.\n","        #       The cluster indices range from 0 to n_clusters - 1.\n","        \"\"\"print(\"X shape:\", X.shape)\n","        print(\"centroids type:\", type(centroids))\n","        print(\"centroids shape:\", centroids.shape)\n","        print(\"hhi\")\"\"\"\n","\n","        distances = np.sqrt(((X[:, np.newaxis] - centroids) ** 2).sum(axis=2))\n","        labels = np.argmin(distances, axis=1)\n","        ###########################################\n","\n","        ###########################################\n","        # TODO: Step 3: Compute new centroids as the mean of the data points in each cluster\n","        # Hint: new_centroids is a numpy array of shape (n_clusters, n_features), where n_clusters is the number of clusters and n_features\n","        #       is the number of features in the input data X. Each row in new_centroids represents the coordinates of a centroid in the feature space.\n","        #       The centroids are computed as the mean of the samples in X that are assigned to each cluster.\n","        new_centroids = np.array([X[labels==i].mean(axis=0) for i in range(n_clusters)])\n","        \"\"\"print(\"new centroids has type:\",type(new_centroids))\n","        print(new_centroids)\"\"\"\n","        ###########################################\n","\n","        ###########################################\n","        # TODO: Step 4: Check for convergence (i.e., no change in the centroids)\n","        if np.all(new_centroids==centroids):\n","            break\n","        centroids = np.copy(new_centroids)\n","        \"\"\"print(type(centroids))\n","        print(centroids)\"\"\"\n","        ###########################################\n","\n","    for i in range(len(centroids)):\n","      results['centroid_'+str(i)] = np.round(centroids[i], 3)\n","\n","    return results"],"metadata":{"id":"xSSUIJHjQKGV","executionInfo":{"status":"aborted","timestamp":1709542532998,"user_tz":-480,"elapsed":3,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__==\"__main__\":\n","  # Load data\n","  subset = data_df[['age', 'high_blood_pressure', 'serum_creatinine', 'serum_sodium', 'smoking']].values\n","\n","  # Run K-Means\n","  results = kmeans(subset, 3)\n","  print(results)"],"metadata":{"id":"Y_b4rwrtSjq7","executionInfo":{"status":"aborted","timestamp":1709542532998,"user_tz":-480,"elapsed":3,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Code for (2) & (3)"],"metadata":{"id":"dajSdbSMUS4l"}},{"cell_type":"markdown","source":[],"metadata":{"id":"17-tETTa-aoM"}},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","\n","def run_kmeans(X,n_clusters, init_method):\n","    results = {}\n","    ###########################################\n","    # TODO: Initialize the KMeans object with the given number of clusters and initialization method\n","    # Note: Set random_state parameter to 42 to ensure the output of function deterministic\n","\n","    kmeans = KMeans(n_clusters=n_clusters,random_state=42,init=init_method)\n","    ###########################################\n","\n","    kmeans.fit(X)\n","\n","    ###########################################\n","    # TODO: Retrieve the cluster centroids\n","    centroids = kmeans.cluster_centers_\n","    ###########################################\n","\n","    for i in range(len(centroids)):\n","      results['centroid_'+str(i)] = np.round(centroids[i], 3)\n","    return results"],"metadata":{"id":"TX-vtOZfSuFg","executionInfo":{"status":"aborted","timestamp":1709542532998,"user_tz":-480,"elapsed":3,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__==\"__main__\":\n","  # Run K-Means with 'random' initialization\n","  results = run_kmeans(subset,3, 'random')\n","  print(results)"],"metadata":{"id":"XL6BcaONUVhC","executionInfo":{"status":"aborted","timestamp":1709542532998,"user_tz":-480,"elapsed":3,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__==\"__main__\":\n","  # Run K-Means with 'random' initialization\n","  results = run_kmeans(subset,3, 'k-means++')\n","  print(results)"],"metadata":{"id":"8GzHiLF8UbbS","executionInfo":{"status":"aborted","timestamp":1709542532998,"user_tz":-480,"elapsed":3,"user":{"displayName":"Zhongli Chung","userId":"16664494979031438955"}}},"execution_count":null,"outputs":[]}],"metadata":{"interpreter":{"hash":"8ae69442b1fc44f14b5ed9e4e267de3f8165086ea0eeb0bf8dc28231d964179e"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[{"file_id":"1v2-WYFxv0RNedcPRygBnGSzjs821c7Gm","timestamp":1700230750448}]}},"nbformat":4,"nbformat_minor":0}